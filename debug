ssh://linzhijie@60.191.10.76:60008/home/linzhijie/anaconda3/bin/python3 -u /home/linzhijie/code/aaai-2020-final/train.py --config-path config/activitynet/main.json
{'dataset': {'dataset': 'ActivityNet', 'feature_path': '/home/linzhijie/data/activity-c3d', 'word_dim': 300, 'frame_dim': 500, 'max_num_words': 20, 'max_num_frames': 200, 'target_stride': 1, 'train_data': 'data/activitynet/train_data.json', 'test_data': 'data/activitynet/test_data.json', 'val_data': 'data/activitynet/val_data.json', 'vocab_path': 'data/activitynet/vocab.pkl', 'prop_width': [0.167, 0.333, 0.5, 0.667, 0.834, 1.0, 1.0, 1.0]}, 'train': {'scp': {'lr': 0.0004, 'weight_decay': 1e-07, 'warmup_updates': 200, 'warmup_init_lr': 1e-07}, 'pg': {'lr': 0.0006, 'weight_decay': 1e-07, 'warmup_updates': 400, 'warmup_init_lr': 1e-07}, 'batch_size': 128, 'max_num_epochs': 20, 'model_saved_path': 'checkpoints/activitynet/main', 'num_proposals': 3, 'rewards': [0, 0.5, 1.0]}, 'model': {'name': 'MainModel', 'config': {'frames_input_size': 500, 'words_input_size': 300, 'hidden_size': 256, 'DualTransformer': {'d_model': 256, 'num_heads': 8, 'num_decoder_layers1': 3, 'num_decoder_layers2': 3, 'dropout': 0.1}}}}
prop width [0.125 0.25  0.375 0.5   0.625 0.75  0.875 1.   ]
prop width [0.125 0.25  0.375 0.5   0.625 0.75  0.875 1.   ]
prop width [0.125 0.25  0.375 0.5   0.625 0.75  0.875 1.   ]
2020-02-14 11:19:11,519 - train: 37421 samples, test: 17031 samples
2020-02-14 11:19:11,520 - GPU: [0]
MainModel(
  (frame_fc): Linear(in_features=500, out_features=256, bias=True)
  (word_fc): Linear(in_features=300, out_features=256, bias=True)
  (trans): DualTransformer(
    (decoder1): TransformerDecoder(
      (decoder_layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (encoder_attn): MultiheadAttention(
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=256, out_features=512, bias=True)
          (fc2): Linear(in_features=512, out_features=256, bias=True)
          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (encoder_attn): MultiheadAttention(
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=256, out_features=512, bias=True)
          (fc2): Linear(in_features=512, out_features=256, bias=True)
          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (encoder_attn): MultiheadAttention(
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=256, out_features=512, bias=True)
          (fc2): Linear(in_features=512, out_features=256, bias=True)
          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (decoder2): TransformerDecoder(
      (decoder_layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (encoder_attn): MultiheadAttention(
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=256, out_features=512, bias=True)
          (fc2): Linear(in_features=512, out_features=256, bias=True)
          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (encoder_attn): MultiheadAttention(
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=256, out_features=512, bias=True)
          (fc2): Linear(in_features=512, out_features=256, bias=True)
          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (encoder_attn): MultiheadAttention(
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=256, out_features=512, bias=True)
          (fc2): Linear(in_features=512, out_features=256, bias=True)
          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (fc_props): Linear(in_features=256, out_features=8, bias=True)
  (fc_comp): Linear(in_features=256, out_features=8001, bias=True)
)
2020-02-14 11:19:19,674 - Start Epoch 1
2020-02-14 11:19:59,220 - Epoch 1, Batch 50, lr = 0.00008, nll = 8.5018, neg_nll = 0.0000, prop = 0.5522, 0.791 seconds/batch
2020-02-14 11:20:34,906 - Epoch 1, Batch 100, lr = 0.00015, nll = 6.9536, neg_nll = 0.0000, prop = 0.5507, 0.752 seconds/batch
2020-02-14 11:21:10,779 - Epoch 1, Batch 150, lr = 0.00023, nll = 5.8909, neg_nll = 0.0000, prop = 0.5499, 0.741 seconds/batch
2020-02-14 11:21:46,999 - Epoch 1, Batch 200, lr = 0.00030, nll = 5.4142, neg_nll = 0.0000, prop = 0.5496, 0.737 seconds/batch
2020-02-14 11:22:23,801 - Epoch 1, Batch 250, lr = 0.00038, nll = 5.1019, neg_nll = 0.0000, prop = 0.5491, 0.737 seconds/batch
2020-02-14 11:22:54,718 - Epoch 1, Batch 293, lr = 0.00044, nll = 4.9336, neg_nll = 0.0000, prop = 0.5493, 0.734 seconds/batch
2020-02-14 11:22:54,789 - save model to checkpoints/activitynet/main/model-1.pt, num_updates 293.
| mIoU 0.2890 | IoU@0.1 0.6469 | IoU@0.3 0.4295 | IoU@0.5 0.2480 | IoU@0.7 0.1112 | IoU@0.9 0.0087 |
2020-02-14 11:24:41,633 - ============================================================
2020-02-14 11:24:41,633 - Start Epoch 2
2020-02-14 11:25:16,267 - Epoch 2, Batch 50, lr = 0.00051, nll = 4.7415, neg_nll = 0.0000, prop = 0.5493, 0.693 seconds/batch
2020-02-14 11:25:49,653 - Epoch 2, Batch 100, lr = 0.00059, nll = 4.6042, neg_nll = 0.0000, prop = 0.5489, 0.680 seconds/batch
2020-02-14 11:26:23,440 - Epoch 2, Batch 150, lr = 0.00057, nll = 4.5111, neg_nll = 0.0000, prop = 0.5489, 0.679 seconds/batch
2020-02-14 11:27:00,212 - Epoch 2, Batch 200, lr = 0.00054, nll = 4.4151, neg_nll = 0.0000, prop = 0.5488, 0.693 seconds/batch
2020-02-14 11:27:34,488 - Epoch 2, Batch 250, lr = 0.00051, nll = 4.3518, neg_nll = 0.0000, prop = 0.5488, 0.691 seconds/batch
2020-02-14 11:28:02,630 - Epoch 2, Batch 293, lr = 0.00050, nll = 4.2792, neg_nll = 0.0000, prop = 0.5485, 0.686 seconds/batch
2020-02-14 11:28:02,685 - save model to checkpoints/activitynet/main/model-2.pt, num_updates 586.
| mIoU 0.3080 | IoU@0.1 0.6687 | IoU@0.3 0.4485 | IoU@0.5 0.2858 | IoU@0.7 0.1248 | IoU@0.9 0.0087 |
2020-02-14 11:29:52,673 - ============================================================
2020-02-14 11:29:52,673 - Start Epoch 3
2020-02-14 11:30:28,934 - Epoch 3, Batch 50, lr = 0.00048, nll = 4.1346, neg_nll = 0.0000, prop = 0.5481, 0.725 seconds/batch
2020-02-14 11:31:05,723 - Epoch 3, Batch 100, lr = 0.00046, nll = 4.1043, neg_nll = 0.0000, prop = 0.5484, 0.730 seconds/batch
2020-02-14 11:31:42,212 - Epoch 3, Batch 150, lr = 0.00044, nll = 4.0922, neg_nll = 0.0000, prop = 0.5480, 0.730 seconds/batch
2020-02-14 11:32:19,467 - Epoch 3, Batch 200, lr = 0.00043, nll = 4.0698, neg_nll = 0.0000, prop = 0.5482, 0.734 seconds/batch
2020-02-14 11:32:55,318 - Epoch 3, Batch 250, lr = 0.00042, nll = 4.0419, neg_nll = 0.0000, prop = 0.5475, 0.731 seconds/batch
2020-02-14 11:33:23,538 - Epoch 3, Batch 293, lr = 0.00040, nll = 4.0011, neg_nll = 0.0000, prop = 0.5484, 0.720 seconds/batch
2020-02-14 11:33:23,593 - save model to checkpoints/activitynet/main/model-3.pt, num_updates 879.
| mIoU 0.3012 | IoU@0.1 0.6579 | IoU@0.3 0.4430 | IoU@0.5 0.2782 | IoU@0.7 0.1227 | IoU@0.9 0.0080 |
2020-02-14 11:35:09,792 - ============================================================
2020-02-14 11:35:09,793 - Start Epoch 4
2020-02-14 11:35:45,556 - Epoch 4, Batch 50, lr = 0.00039, nll = 3.8883, neg_nll = 0.0000, prop = 0.5476, 0.715 seconds/batch
